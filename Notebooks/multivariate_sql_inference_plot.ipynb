{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Anomaly Detection Demo Notebook\n",
    "\n",
    "IoT Hub -> SQL -> Notebook -> Anomaly Detector API -> Chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Introduction](#intro)\n",
    "2. [Prerequisites](#pre)\n",
    "5. [Inference](#inference)\n",
    "6. [Analysis (for reference only)](#analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prerequisites <a class=\"anchor\" id=\"pre\"></a>\n",
    "\n",
    "\n",
    "* [Create an Azure subscription](https://azure.microsoft.com/free/cognitive-services) if you don't have one.\n",
    "* [Create an Anomaly Detector resource](https://ms.portal.azure.com/#create/Microsoft.CognitiveServicesAnomalyDetector) and get your `endpoint` and `key`, you'll use these later.\n",
    "* (**optional**) [Install Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli) A helpful tool to manipulate your Azure resources. You can use Azure CLI to retrieve credential information without pasting them as plain text.\n",
    "* (**optional**) Login with Azure CLI `az login`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample code to generate SAS (for reference only)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization of detection results (for reference only) <a class=\"anchor\" id=\"analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select device for anomaly analysis\n",
    "\n",
    "deviceId = \"machine1\"\n",
    "inferenceDays = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages. uncomment to install\n",
    "\n",
    "# ! pip3 install notebook\n",
    "# ! pip3 install azure-ai-anomalydetector\n",
    "# ! pip3 install azure-core\n",
    "# ! pip3 install azure-storage-blob\n",
    "# ! pip3 install python-dotenv\n",
    "# ! pip3 install pandas\n",
    "# ! pip3 install plotly\n",
    "# ! pip3 install bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "from azure.ai.anomalydetector import AnomalyDetectorClient\n",
    "from azure.ai.anomalydetector.models import DetectionRequest, DetectionStatus\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "from azure.storage.blob import BlobClient, BlobServiceClient, generate_blob_sas, BlobSasPermissions\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import time\n",
    "import zipfile\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "\n",
    "env_path = Path('.') / '.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "inference_telemetry_endpoint_url = os.environ.get('inference_telemetry_endpoint_url')\n",
    "inference_telemetry_endpoint_key = os.environ.get('inference_telemetry_endpoint_key')\n",
    "storage_connection_string = os.environ.get('storage_connection_string')\n",
    "anomaly_detector_endpoint = os.environ.get('anomaly_detector_endpoint')\n",
    "anomaly_detector_key = os.environ.get('anomaly_detector_key')\n",
    "model_id = os.environ.get('anomaly_detector_model_id')\n",
    "\n",
    "temp_dir = tempfile.gettempdir()\n",
    "zip_filename = temp_dir + \"/telemetry_mvad.zip\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Azure Anomaly Detector helper functions\n",
    "\n",
    "class MultivariateSample:\n",
    "\n",
    "    def __init__(self, anomaly_detector_endpoint=None, anomaly_detector_key=None, model_id=None, connection_string=None, container=None, blob_name=None):\n",
    "        self.blob_name = blob_name\n",
    "        self.container = container\n",
    "        self.connection_string = connection_string\n",
    "        self.model_id = model_id\n",
    "        self.anomaly_detector_endpoint = anomaly_detector_endpoint\n",
    "        self.anomaly_detector_key = anomaly_detector_key\n",
    "\n",
    "        # Create an Anomaly Detector client\n",
    "\n",
    "        # <client>\n",
    "        self.ad_client = AnomalyDetectorClient(AzureKeyCredential(self.anomaly_detector_key), self.anomaly_detector_endpoint)\n",
    "        # </client>        \n",
    "\n",
    "    def upload_blob(self, filename):\n",
    "        blob_client = BlobClient.from_connection_string(self.connection_string, container_name=self.container, blob_name=self.blob_name)\n",
    "        with open(filename, \"rb\") as f:\n",
    "            blob_client.upload_blob(f, overwrite=True)\n",
    "\n",
    "    def detect(self, start_time, end_time):\n",
    "        # Detect anomaly in the same data source (but a different interval)\n",
    "        try:\n",
    "            data_source = self.generate_data_source_sas(self.container, self.blob_name)\n",
    "            detection_req = DetectionRequest(source=data_source, start_time=start_time, end_time=end_time)\n",
    "            response_header = self.ad_client.detect_anomaly(self.model_id, detection_req,\n",
    "                                                            cls=lambda *args: [args[i] for i in range(len(args))])[-1]\n",
    "            result_id = response_header['Location'].split(\"/\")[-1]\n",
    "\n",
    "            # Get results (may need a few seconds)\n",
    "            r = self.ad_client.get_detection_result(result_id)\n",
    "            print(\"Get detection result...(it may take a few seconds)\")\n",
    "\n",
    "            while r.summary.status != DetectionStatus.READY and r.summary.status != DetectionStatus.FAILED:\n",
    "                r = self.ad_client.get_detection_result(result_id)\n",
    "                print(\"waiting for anomaly detection result...\")\n",
    "                time.sleep(1)\n",
    "\n",
    "            if r.summary.status == DetectionStatus.FAILED:\n",
    "                print(\"Detection failed.\")\n",
    "                if r.summary.errors:\n",
    "                    for error in r.summary.errors:\n",
    "                        print(\"Error code: {}. Message: {}\".format(error.code, error.message))\n",
    "                else:\n",
    "                    print(\"None\")\n",
    "                return None\n",
    "\n",
    "        except HttpResponseError as e:\n",
    "            print('Error code: {}'.format(e.error.code), 'Error message: {}'.format(e.error.message))\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "        return r\n",
    "\n",
    "    def generate_data_source_sas(self, container, blob_name):\n",
    "        BLOB_SAS_TEMPLATE = \"{blob_endpoint}{container_name}/{blob_name}?{sas_token}\"\n",
    "\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(conn_str=self.connection_string)\n",
    "        sas_token = generate_blob_sas(account_name=blob_service_client.account_name,\n",
    "                                    container_name=container, blob_name=blob_name,\n",
    "                                    account_key=blob_service_client.credential.account_key,\n",
    "                                    permission=BlobSasPermissions(read=True),\n",
    "                                    expiry=datetime.utcnow() + timedelta(days=1))\n",
    "        blob_sas = BLOB_SAS_TEMPLATE.format(blob_endpoint=blob_service_client.primary_endpoint,\n",
    "                                            container_name=container, blob_name=blob_name, sas_token=sas_token)\n",
    "        return blob_sas\n",
    "    \n",
    "\n",
    "sample = MultivariateSample(anomaly_detector_endpoint, anomaly_detector_key, model_id, storage_connection_string, 'data', \"telemetry_mvad.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get last X days of telemetry from Azure SQL via REST API\n",
    "\n",
    "api_url = f\"{inference_telemetry_endpoint_url}/{deviceId}/{inferenceDays}?code={inference_telemetry_endpoint_key}\"\n",
    "\n",
    "df = pd.read_json(api_url, convert_dates=False)\n",
    "df.set_index('timestamp', inplace=True) \n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload batch of data tp Azure Storage Account for inference\n",
    "\n",
    "if not df.empty:    \n",
    "\n",
    "    zip_file = zipfile.ZipFile(zip_filename, \"w\", zipfile.ZIP_DEFLATED)\n",
    "\n",
    "    for variable in df.columns:\n",
    "        individual_df = pd.DataFrame(df[variable].values, index=df.index, columns=[\"value\"])\n",
    "        individual_df.to_csv(temp_dir + \"/\" + variable + \".csv\", index=True)\n",
    "        zip_file.write(temp_dir + \"/\" + variable + \".csv\", arcname=variable + \".csv\")\n",
    "\n",
    "    zip_file.close()\n",
    "\n",
    "    sample.upload_blob(zip_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Azure Anomaly Detector API\n",
    "\n",
    "if not df.empty:\n",
    "    \n",
    "    start_time = df.index[0]\n",
    "    end_time = df.index[-1]\n",
    "\n",
    "    r = sample.detect(start_time=start_time, end_time=end_time)\n",
    "\n",
    "    if r is not None:\n",
    "        results = r.results\n",
    "        print(\"Anomaly detection completed\")\n",
    "    else:\n",
    "        print(\"Anomaly detection failed\")\n",
    "        results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build chart data\n",
    "\n",
    "if results is not None:\n",
    "\n",
    "    is_anomalies = []\n",
    "    sev = []\n",
    "    scores = []\n",
    "    sensitivity = 0.15\n",
    "\n",
    "    for item in results:\n",
    "        if item.value:\n",
    "            is_anomalies.append(item.value.is_anomaly)\n",
    "            sev.append(item.value.severity)\n",
    "            scores.append(item.value.score)\n",
    "\n",
    "    anomalous_timestamps = []\n",
    "    num_contributors = 3\n",
    "    top_values = {f\"top_{i}\": [] for i in range(num_contributors)}\n",
    "    \n",
    "    for ts, item in zip(df.index, r.results):\n",
    "        if item.value.is_anomaly and item.value.severity > 1 - sensitivity:\n",
    "            anomalous_timestamps.append(ts)\n",
    "            for i in range(num_contributors):\n",
    "                top_values[f\"top_{i}\"].append(df[item.value.interpretation[i].variable][ts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart results from Azure Anomaly Detector API\n",
    "\n",
    "if results is not None:\n",
    "    print(\"Display chart\")\n",
    "\n",
    "    fig = make_subplots(rows=3, cols=1, shared_xaxes=True)\n",
    "    colors = [px.colors.sequential.Greys[-1], px.colors.sequential.Greys[-3], px.colors.sequential.Greys[-6]]\n",
    "    \n",
    "    for v in df.columns:\n",
    "        fig.add_trace(go.Scatter(x=df.index, y=df[v], mode='lines', name=v), row=1, col=1)\n",
    "        \n",
    "    for i in range(num_contributors):\n",
    "        fig.add_trace(go.Scatter(x=anomalous_timestamps, y=top_values[f\"top_{i}\"],\n",
    "                                 mode=\"markers\", name=f\"Top {i+1} contributor\",\n",
    "                                 marker=dict(color=colors[i],size=8,)),row=1, col=1)\n",
    "        \n",
    "    fig.add_trace(go.Scatter(x=df.index, y=scores, mode='lines', name='score'), row=2, col=1)\n",
    "    fig.add_trace(go.Scatter(x=df.index, y=sev, mode='lines', name='severity'), row=3, col=1)\n",
    "    \n",
    "    fig.update_layout(title_text=\"Visualization of detection results\")\n",
    "    fig.update_yaxes(title_text=\"value\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"score\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"severity\", row=3, col=1)\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e66d3849ffa053565a586dec38b3bf28a35d140e923d8fe0a14c615c657862cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
