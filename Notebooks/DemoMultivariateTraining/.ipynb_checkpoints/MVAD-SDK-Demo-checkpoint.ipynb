{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Anomaly Detection Demo Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Introduction](#intro)\n",
    "2. [Prerequisites](#pre)\n",
    "3. [Train a Model](#train)\n",
    "4. [List Models](#list)\n",
    "5. [Inference](#inference)\n",
    "6. [Analysis (for reference only)](#analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introdution <a class=\"anchor\" id=\"intro\"></a>\n",
    "This notebook shows how to use [Multivariate Anomaly Detection](https://docs.microsoft.com/en-us/azure/cognitive-services/anomaly-detector/overview-multivariate) in Anomaly Detector service. Please follow the steps to try it out, you can either [join Teams Group](https://forms.office.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxSkyhztUNZCtaivu8nmhd1UQ1VFRDA0V1dUMDJRMFhOTzFHQ1lDTVozWi4u) for any questions, or email us via AnomalyDetector@microsoft.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prerequisites <a class=\"anchor\" id=\"pre\"></a>\n",
    "\n",
    "\n",
    "* [Create an Azure subscription](https://azure.microsoft.com/free/cognitive-services) if you don't have one.\n",
    "* [Create an Anomaly Detector resource](https://ms.portal.azure.com/#create/Microsoft.CognitiveServicesAnomalyDetector) and get your `endpoint` and `key`, you'll use these later.\n",
    "* (**optional**) [Install Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli) A helpful tool to manipulate your Azure resources. You can use Azure CLI to retrieve credential information without pasting them as plain text.\n",
    "* (**optional**) Login with Azure CLI `az login`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Export the following environment variables\n",
    "\n",
    "BLOB_SAS_TEMPLATE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Install** the anomaly detector SDK and storage packages using following codes ‚¨áÔ∏è, and **import** packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOB_SAS_TEMPLATE = \"\"\n",
    "\n",
    "STORAGE_ACCOUNT_CONNECTION_STRING = \"\"\n",
    "\n",
    "AD_SUBSCRIPTION_KEY = \"\"\n",
    "AD_ENDPOINT_URL = \"\"\n",
    "\n",
    "\n",
    "\n",
    "# AD_SUBSCRIPTION_KEY = os.getenv(\"SUB_KEY\")\n",
    "# AD_ENDPOINT_URL = os.getenv(\"AD_ENDPOINT\")\n",
    "\n",
    "# STORAGE_ACCOUNT_CONNECTION_STRING = os.getenv(\"STORAGE_ACCOUNT_CONNECTION_STRING\")\n",
    "# print(STORAGE_ACCOUNT_CONNECTION_STRING)\n",
    "\n",
    "# # or\n",
    "STORAGE_ACCOUNT_NAME = \"\"   # storage account name\n",
    "STORAGE_ACCOUNT_RESOURCE_GROUP = \"\"  # resource group\n",
    "\n",
    "ZIP_FILENAME = \"telemetry_mvad.zip\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Install required packages. Use the following commands to install the anomaly detector SDK and required packages.\n",
    "# ! pip install --upgrade azure-ai-anomalydetector\n",
    "# ! pip install azure-storage-blob\n",
    "# ! pip install azure-mgmt-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install optional packages to see interactive visualization in this Jupyter notebook.\n",
    "# ! pip install plotly==5.5.0\n",
    "# ! pip install notebook>=5.3 \n",
    "# ! pip install ipywidgets>=7.5\n",
    "# ! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import related packages:\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "from datetime import datetime\n",
    "from azure.ai.anomalydetector import AnomalyDetectorClient\n",
    "from azure.ai.anomalydetector.models import DetectionRequest, ModelInfo, LastDetectionRequest, VariableValues\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# This is to build interactive plot:\n",
    "pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We will use a simulated dataset **([multivariate_sample_data.csv](https://github.com/Azure-Samples/AnomalyDetector/blob/master/ipython-notebook/SDK%20Sample/multivariate_sample_data.csv))** in the Github repository. This dataset contains five variables which represent different variables from an equipment.\n",
    "\n",
    "If you'd like to use your own dataset to run this notebook, you should do the following steps first (üé¨[video instruction](https://msit.microsoftstream.com/video/afa00840-98dc-ae72-fad1-f1ec0fe830c1)/[video backup](https://github.com/Azure-Samples/AnomalyDetector/blob/master/ipython-notebook/media/How%20to%20generate%20a%20SAS.mp4)):\n",
    "1. (optional) Split your full csv files into individual csv files that each file contains the data for one variable.\n",
    "1. Compress your local csv files(one metric per file), see [input data schema](https://docs.microsoft.com/en-us/azure/cognitive-services/anomaly-detector/concepts/best-practices-multivariate#input-data-schema)..\n",
    "1. Upload the compressed file to Azure Blob.\n",
    "1. Generate an `SAS URL` for your compressed file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data visualization\n",
    "df = pd.read_csv(\"./training/sensors.csv\", index_col=\"timestamp\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's draw an interactive plot. You may zoom in/out through clicking 'autoscale' and select an area or select a variabe for further investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = \"2022-07-18T00:00:00Z\"\n",
    "end_time = \"2022-07-26T23:59:00Z\"\n",
    "df[(df.index > start_time) & (df.index < end_time)].plot(title='Sample data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample code to generate SAS (for reference only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.storage.blob import BlobClient, BlobServiceClient, generate_blob_sas, BlobSasPermissions\n",
    "from datetime import datetime, timedelta\n",
    "import zipfile\n",
    "\n",
    "def zip_file(root, name):\n",
    "    \"\"\"\n",
    "    A helper function to compress local csv files.\n",
    "    :param root: root directory of csv files\n",
    "    :param name: name of the compressed file (with suffix) \n",
    "    \"\"\"\n",
    "    z = zipfile.ZipFile(name, \"w\", zipfile.ZIP_DEFLATED)\n",
    "    for f in os.listdir(root):\n",
    "        if f.endswith(\"csv\"):\n",
    "            z.write(os.path.join(root, f), f)\n",
    "    z.close()\n",
    "    print(\"Compress files success!\")\n",
    "\n",
    "\n",
    "def upload_to_blob(file, conn_str, container, blob_name):\n",
    "    \"\"\"\n",
    "    A helper function to upload files to blob\n",
    "    :param file: the path to the file to be uploaded\n",
    "    :param conn_str: the connection string of the target storage account\n",
    "    :param container: the container name in the storage account\n",
    "    :param blob_name: the blob name in the container\n",
    "    \"\"\"\n",
    "    blob_client = BlobClient.from_connection_string(conn_str, container_name=container, blob_name=blob_name)\n",
    "    with open(file, \"rb\") as f:\n",
    "        blob_client.upload_blob(f, overwrite=True)\n",
    "    print(\"Upload Success!\")\n",
    "\n",
    "\n",
    "def generate_data_source_sas(conn_str, container, blob_name):\n",
    "    \"\"\"\n",
    "    A helper function to generate blob SAS.\n",
    "    :param conn_str: the connection string of the target storage account\n",
    "    :param container: the container name in the storage account\n",
    "    :param blob_name: the blob name in the container\n",
    "    :return: generated SAS\n",
    "    \"\"\"\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(conn_str=conn_str)\n",
    "    sas_token = generate_blob_sas(account_name=blob_service_client.account_name,\n",
    "                                  container_name=container,\n",
    "                                  blob_name=blob_name,\n",
    "                                  account_key=blob_service_client.credential.account_key,\n",
    "                                  permission=BlobSasPermissions(read=True),\n",
    "                                  expiry=datetime.utcnow() + timedelta(days=1))\n",
    "    blob_sas = BLOB_SAS_TEMPLATE.format(account_name=blob_service_client.account_name,\n",
    "                                        container_name=container,\n",
    "                                        blob_name=blob_name,\n",
    "                                        sas_token=sas_token)\n",
    "    return blob_sas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "account_name = STORAGE_ACCOUNT_NAME   # storage account name\n",
    "resource_group = STORAGE_ACCOUNT_RESOURCE_GROUP  # resource group\n",
    "try:\n",
    "    cmd = f\"az storage account keys list -g {resource_group} -n {account_name}\"   # using az-cli is safer\n",
    "    az_response = subprocess.run(cmd.split(\" \"), stdout=subprocess.PIPE).stdout\n",
    "    key = json.loads(az_response)[0][\"value\"]\n",
    "    connection_string = f\"DefaultEndpointsProtocol=https;AccountName={account_name};AccountKey={key};EndpointSuffix=core.windows.net\"\n",
    "except FileNotFoundError:    # no az-cli available\n",
    "    connection_string = STORAGE_ACCOUNT_CONNECTION_STRING\n",
    "container_name = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset\n",
    "source_folder = \"tmp_csvs\"\n",
    "zipfile_name = ZIP_FILENAME\n",
    "\n",
    "os.makedirs(source_folder, exist_ok=True)\n",
    "for variable in df.columns:\n",
    "    individual_df = pd.DataFrame(df[variable].values, index=df.index, columns=[\"value\"])\n",
    "    individual_df.to_csv(os.path.join(source_folder, f\"{variable}.csv\"))\n",
    "    \n",
    "zip_file(source_folder, zipfile_name)\n",
    "\n",
    "# Remove the temporary directory created for splitting out the features\n",
    "shutil.rmtree(source_folder)\n",
    "\n",
    "upload_to_blob(zipfile_name, connection_string, container_name, zipfile_name)\n",
    "data_source = generate_data_source_sas(connection_string, container_name, zipfile_name)\n",
    "print(\"Blob SAS url: \" + data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train a model <a class=\"anchor\" id=\"train\"></a>\n",
    "\n",
    "Before you train a model, you should specify the `subscription key` and `endpoint` of your Anomaly Detector service to create an Anomaly Detector client in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After you create an Anomaly Detector resource in Azure portal, you will get the endpoint and key, and put them here.\n",
    "try:\n",
    "    resource_group = STORAGE_ACCOUNT_RESOURCE_GROUP\n",
    "    account_name = STORAGE_ACCOUNT_NAME\n",
    "    cmd = f\"az cognitiveservices account keys list -g {resource_group} -n {account_name}\"   # using az-cli is safer\n",
    "    az_response = subprocess.run(cmd.split(\" \"), stdout=subprocess.PIPE).stdout\n",
    "    subscription_key = json.loads(az_response)[\"key1\"]\n",
    "    anomaly_detector_endpoint = f\"https://{account_name}.cognitiveservices.azure.com\"\n",
    "except FileNotFoundError:\n",
    "    subscription_key = AD_SUBSCRIPTION_KEY\n",
    "    anomaly_detector_endpoint = AD_ENDPOINT_URL\n",
    "# Create an Anomaly Detector client.\n",
    "ad_client = AnomalyDetectorClient(AzureKeyCredential(subscription_key), anomaly_detector_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Specify the timespan of training data using `start_time` and `end_time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = \"2022-07-19T00:00:00Z\"\n",
    "end_time = \"2022-07-26T23:59:00Z\"\n",
    "sliding_window = 50\n",
    "\n",
    "# Hyperparameter of model - controls how much data for input\n",
    "# If the data has natural period eg weekly or daily - fit the data to the pattern with the sliding_window\n",
    "# If data changes rapid then helpful to have a longer sliding window - ideally covering a pattern\n",
    "# If the data doesn't change much then a smaller window is good - reduces processing time\n",
    "# Sliding_window controls min length of data - A sliding window of 28 means you must at least provide 28 data points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feed = ModelInfo(start_time=start_time, end_time=end_time, source=data_source, sliding_window=sliding_window)\n",
    "response_header = ad_client.train_multivariate_model(data_feed, cls=lambda *args: [args[i] for i in range(len(args))])[-1]\n",
    "trained_model_id = response_header['Location'].split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"model id: {trained_model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Model Status\n",
    "‚òïÔ∏èTraining process might take few minutes to few hours (depending on the data size, in this sample case it'll take you within 3 minutes), take a cup of coffee and come back then, waiting for its status to be **READY**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_status = ad_client.get_multivariate_model(trained_model_id).model_info.status\n",
    "print(f\"model status: {model_status}\")\n",
    "\n",
    "while model_status != \"READY\" and model_status != \"FAILED\":\n",
    "    time.sleep(10)\n",
    "    model_status = ad_client.get_multivariate_model(trained_model_id).model_info.status\n",
    "    print(f\"model status: {model_status}\")\n",
    "\n",
    "#If the model status is failed, run the following code to see the error message.\n",
    "# print ([x.code + ' ' + x.message for x in train_response.model_info.errors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ad_client.get_multivariate_model(trained_model_id).model_info.errors[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get model information and track training progress.\n",
    "import numpy as np\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "model = ad_client.get_multivariate_model(trained_model_id)\n",
    "current_epoch = 0 if len(model.model_info.diagnostics_info.model_state.epoch_ids) == 0 else model.model_info.diagnostics_info.model_state.epoch_ids[-1]\n",
    "print(f\"training progress: {current_epoch}/100.\")\n",
    "if model.model_info.status == \"READY\":\n",
    "    model_state = model.model_info.diagnostics_info.model_state\n",
    "    epoch_ids = model_state.epoch_ids\n",
    "    train_losses = model_state.train_losses\n",
    "    validation_losses = model_state.validation_losses\n",
    "    latency = model_state.latencies_in_seconds\n",
    "    loss_summary = pd.DataFrame({\n",
    "        \"epoch_id\": epoch_ids, \n",
    "        \"train_loss\": train_losses, \n",
    "        \"validation_loss\": validation_losses,\n",
    "        \"latency\": latency\n",
    "    })\n",
    "    display(loss_summary)\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "    fig.add_trace(go.Scatter(x=epoch_ids, y=train_losses, \n",
    "                             mode='lines',\n",
    "                             name='train losses'))\n",
    "    fig.add_trace(go.Scatter(x=epoch_ids, y=validation_losses,\n",
    "                             mode='lines',\n",
    "                             name='validation losses'))\n",
    "    fig.add_trace(go.Scatter(x=epoch_ids, y=latency,\n",
    "                             mode='markers', name='latency'),\n",
    "                  secondary_y=True)\n",
    "    fig.update_layout(\n",
    "        title_text=\"Visualization of training progress\"\n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"Epoch IDs\")\n",
    "\n",
    "    # Set y-axes titles\n",
    "    fig.update_yaxes(title_text=\"Loss value\", secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"Latency (s)\", secondary_y=True)\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  List Models <a class=\"anchor\" id=\"list\"></a>\n",
    "List models that have been trained previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = list(ad_client.list_multivariate_model(skip=0, top=100))\n",
    "model_summary = pd.DataFrame([{\"model_id\": m.model_id, \"status\": m.status} for m in model_list[:5]])\n",
    "display(model_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the first model\n",
    "model = model_list[0]\n",
    "vars(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference <a class=\"anchor\" id=\"inference\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Inference asynchronously\n",
    "\n",
    "You should inference first and get a result id, then use the id to get the detection result.\n",
    "- Specify the time span of inference data using `start_time` and `end_time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the start time and end time for inference.\n",
    "start_inference_time = \"2022-07-27T00:00:00Z\"\n",
    "end_inference_time = \"2022-07-27T02:59:00Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_req = DetectionRequest(source=data_source, start_time=start_inference_time, end_time=end_inference_time)\n",
    "response_header = ad_client.detect_anomaly(trained_model_id, detection_req, cls=lambda *args: [args[i] for i in range(len(args))])[-1]\n",
    "result_id = response_header['Location'].split(\"/\")[-1]\n",
    "print(f\"result id: {result_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get inference status\n",
    "‚òïÔ∏èInference process might\n",
    "take 10-20mins (depending on the data size). Take a cup of coffee and come back then, and waiting for its status to be **READY**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r = ad_client.get_detection_result(result_id)\n",
    "print(f\"result status: {r.summary.status}\")\n",
    "\n",
    "while r.summary.status != \"READY\"  and r.summary.status != \"FAILED\":\n",
    "    time.sleep(10)\n",
    "    r = ad_client.get_detection_result(result_id)\n",
    "    print(f\"result status: {r.summary.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Have a look at the first anomalous result of inference.\n",
    "for r_item in r.results:\n",
    "    if r_item.value.is_anomaly:\n",
    "        print(r_item.value)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Inference with the synchronous API (NEW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This synchronous API will get detection result immediately after you call it, you should put your data with JSON format into the request body, and specify how much data points you'd like to detect within the `detectingPoints` field, which could be a number **between 1 and 10**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# sample_input_df = df[df.index<=\"2022-07-27T02:18:00Z\"][-110:]\n",
    "# sample_input = [{\"name\": var, \n",
    "#                  \"timestamps\": sample_input_df.index.tolist(), \n",
    "#                  \"values\": sample_input_df[var].tolist()} for var in sample_input_df.columns]\n",
    "\n",
    "# print(sample_input)\n",
    "# last_detection_request = LastDetectionRequest(variables=[VariableValues(**item) for item in sample_input], detecting_points=10)\n",
    "# res = ad_client.last_detect_anomaly(model_id=trained_model_id, body=last_detection_request)\n",
    "\n",
    "# print(\"return from last detect\")\n",
    "\n",
    "# results = pd.DataFrame(columns=[\"timestamp\", \"is_anomaly\", \"severity\", \"score\"])\n",
    "# for item in res.results:\n",
    "#     results = results.append({\"timestamp\": item.timestamp.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "#                               \"is_anomaly\": item.value.is_anomaly,\n",
    "#                               \"severity\": item.value.severity,\n",
    "#                               \"score\": item.value.score}, ignore_index=True)\n",
    "# display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization of detection results (for reference only) <a class=\"anchor\" id=\"analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = r.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view inference data - Specifies the period to be charted. \n",
    "# Almost certainly lesss than the inference dataset window\n",
    "test_df = df.loc[\"2022-07-27T00:00:00Z\":\"2022-07-27T02:59:00Z\"]\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_anomalies = []\n",
    "sev = []\n",
    "scores = []\n",
    "sensitivity = 0.15\n",
    "for item in results:\n",
    "    if item.value:\n",
    "        is_anomalies.append(item.value.is_anomaly)\n",
    "        sev.append(item.value.severity)\n",
    "        scores.append(item.value.score)\n",
    "\n",
    "anomolous_timestamps = []\n",
    "num_contributors = 3\n",
    "top_values = {f\"top_{i}\": [] for i in range(num_contributors)}\n",
    "for ts, item in zip(test_df.index, r.results):\n",
    "    if item.value.is_anomaly and item.value.severity > 1 - sensitivity:\n",
    "        anomolous_timestamps.append(ts)\n",
    "        for i in range(num_contributors):\n",
    "            top_values[f\"top_{i}\"].append(test_df[item.value.interpretation[i].variable][ts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=3, cols=1, shared_xaxes=True)\n",
    "colors = [px.colors.sequential.Greys[-1], px.colors.sequential.Greys[-3], px.colors.sequential.Greys[-6]]\n",
    "for v in test_df.columns:\n",
    "    fig.add_trace(go.Scatter(x=test_df.index, y=test_df[v], \n",
    "                             mode='lines',\n",
    "                             name=v),\n",
    "                  row=1, col=1)\n",
    "for i in range(num_contributors):\n",
    "    fig.add_trace(go.Scatter(x=anomolous_timestamps, y=top_values[f\"top_{i}\"],\n",
    "                             mode=\"markers\", name=f\"Top {i+1} contributor\",\n",
    "                             marker=dict(\n",
    "                                color=colors[i],\n",
    "                                size=8,\n",
    "                            )),\n",
    "                  row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=test_df.index, y=scores,\n",
    "                         mode='lines',\n",
    "                         name='score'),\n",
    "              row=2, col=1)\n",
    "fig.add_trace(go.Scatter(x=test_df.index, y=sev,\n",
    "                         mode='lines', name='severity'),\n",
    "              row=3, col=1)\n",
    "fig.update_layout(\n",
    "    title_text=\"Visualization of detection results\"\n",
    ")\n",
    "fig.update_yaxes(title_text=\"value\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"score\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"severity\", row=3, col=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e66d3849ffa053565a586dec38b3bf28a35d140e923d8fe0a14c615c657862cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
