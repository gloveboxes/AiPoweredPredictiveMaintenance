{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Anomaly Detection Demo Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Introduction](#intro)\n",
    "2. [Prerequisites](#pre)\n",
    "3. [Train a Model](#train)\n",
    "4. [List Models](#list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introdution <a class=\"anchor\" id=\"intro\"></a>\n",
    "This notebook shows how to use [Multivariate Anomaly Detection](https://docs.microsoft.com/en-us/azure/cognitive-services/anomaly-detector/overview-multivariate) in Anomaly Detector service. Please follow the steps to try it out, you can either [join Teams Group](https://forms.office.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbRxSkyhztUNZCtaivu8nmhd1UQ1VFRDA0V1dUMDJRMFhOTzFHQ1lDTVozWi4u) for any questions, or email us via AnomalyDetector@microsoft.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prerequisites <a class=\"anchor\" id=\"pre\"></a>\n",
    "\n",
    "\n",
    "* [Create an Azure subscription](https://azure.microsoft.com/free/cognitive-services) if you don't have one.\n",
    "* [Create an Anomaly Detector resource](https://ms.portal.azure.com/#create/Microsoft.CognitiveServicesAnomalyDetector) and get your `endpoint` and `key`, you'll use these later.\n",
    "* (**optional**) [Install Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli) A helpful tool to manipulate your Azure resources. You can use Azure CLI to retrieve credential information without pasting them as plain text.\n",
    "* (**optional**) Login with Azure CLI `az login`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Export the following environment variables\n",
    "\n",
    "BLOB_SAS_TEMPLATE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Install** the anomaly detector SDK and storage packages using following codes ‚¨áÔ∏è, and **import** packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Install required packages. Use the following commands to install the anomaly detector SDK and required packages.\n",
    "# ! pip install --upgrade azure-ai-anomalydetector\n",
    "# ! pip install azure-storage-blob\n",
    "# ! pip install azure-mgmt-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install optional packages to see interactive visualization in this Jupyter notebook.\n",
    "# ! pip install plotly==5.5.0\n",
    "# ! pip install notebook>=5.3 \n",
    "# ! pip install ipywidgets>=7.5\n",
    "# ! pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import related packages:\n",
    "\n",
    "from azure.ai.anomalydetector import AnomalyDetectorClient\n",
    "from azure.ai.anomalydetector.models import DetectionRequest, ModelInfo, DetectionStatus\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "from azure.storage.blob import BlobClient, BlobServiceClient, generate_blob_sas, BlobSasPermissions\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import time\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "\n",
    "env_path = Path('..') / '.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "storage_connection_string = os.environ.get('storage_connection_string')\n",
    "anomaly_detector_endpoint = os.environ.get('anomaly_detector_endpoint')\n",
    "anomaly_detector_key = os.environ.get('anomaly_detector_key')\n",
    "\n",
    "temp_dir = tempfile.gettempdir()\n",
    "blob_name = \"training_mvad.zip\"\n",
    "model_id = ''\n",
    "\n",
    "zip_filename = temp_dir + blob_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We will use a simulated dataset **([multivariate_sample_data.csv](https://github.com/Azure-Samples/AnomalyDetector/blob/master/ipython-notebook/SDK%20Sample/multivariate_sample_data.csv))** in the Github repository. This dataset contains five variables which represent different variables from an equipment.\n",
    "\n",
    "If you'd like to use your own dataset to run this notebook, you should do the following steps first (üé¨[video instruction](https://msit.microsoftstream.com/video/afa00840-98dc-ae72-fad1-f1ec0fe830c1)/[video backup](https://github.com/Azure-Samples/AnomalyDetector/blob/master/ipython-notebook/media/How%20to%20generate%20a%20SAS.mp4)):\n",
    "1. (optional) Split your full csv files into individual csv files that each file contains the data for one variable.\n",
    "1. Compress your local csv files(one metric per file), see [input data schema](https://docs.microsoft.com/en-us/azure/cognitive-services/anomaly-detector/concepts/best-practices-multivariate#input-data-schema)..\n",
    "1. Upload the compressed file to Azure Blob.\n",
    "1. Generate an `SAS URL` for your compressed file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data visualization\n",
    "df = pd.read_csv(\"./training/sensors.csv\", index_col=\"timestamp\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's draw an interactive plot. You may zoom in/out through clicking 'autoscale' and select an area or select a variabe for further investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample code to generate SAS (for reference only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Azure Anomaly Detector helper functions\n",
    "\n",
    "class MultivariateSample:\n",
    "\n",
    "    def __init__(self, anomaly_detector_endpoint=None, anomaly_detector_key=None, model_id=None, connection_string=None, container=None, blob_name=None):\n",
    "        self.blob_name = blob_name\n",
    "        self.container = container\n",
    "        self.connection_string = connection_string\n",
    "        self.model_id = model_id\n",
    "        self.anomaly_detector_endpoint = anomaly_detector_endpoint\n",
    "        self.anomaly_detector_key = anomaly_detector_key\n",
    "\n",
    "        # Create an Anomaly Detector client\n",
    "\n",
    "        # <client>\n",
    "        self.ad_client = AnomalyDetectorClient(AzureKeyCredential(self.anomaly_detector_key), self.anomaly_detector_endpoint)\n",
    "        # </client>        \n",
    "\n",
    "    def zip_data(self, df, zip_filename):\n",
    "        # Zip data files\n",
    "        zip_file = zipfile.ZipFile(zip_filename, \"w\", zipfile.ZIP_DEFLATED)\n",
    "\n",
    "        for variable in df.columns:\n",
    "            individual_df = pd.DataFrame(df[variable].values, index=df.index, columns=[\"value\"])\n",
    "            individual_df.to_csv(temp_dir + \"/\" + variable + \".csv\", index=True)\n",
    "            zip_file.write(temp_dir + \"/\" + variable + \".csv\", arcname=variable + \".csv\")\n",
    "\n",
    "        zip_file.close()\n",
    "\n",
    "    def upload_blob(self, filename):\n",
    "        blob_client = BlobClient.from_connection_string(self.connection_string, container_name=self.container, blob_name=self.blob_name)\n",
    "        with open(filename, \"rb\") as f:\n",
    "            blob_client.upload_blob(f, overwrite=True)\n",
    "\n",
    "    def train_model(self, start_time, end_time, sliding_window):\n",
    "        data_source = self.generate_data_source_sas(self.container, self.blob_name)\n",
    "        data_feed = ModelInfo(start_time=start_time, end_time=end_time, source=data_source, sliding_window=sliding_window)\n",
    "        response_header = self.ad_client.train_multivariate_model(data_feed, cls=lambda *args: [args[i] for i in range(len(args))])[-1]\n",
    "        trained_model_id = response_header['Location'].split(\"/\")[-1]\n",
    "        print(f\"model id: {trained_model_id}\")\n",
    "\n",
    "        model_status = self.ad_client.get_multivariate_model(trained_model_id).model_info.status\n",
    "        print(f\"model status: {model_status}\")\n",
    "\n",
    "        while model_status != \"READY\" and model_status != \"FAILED\":   \n",
    "            time.sleep(10)         \n",
    "            model_status = self.ad_client.get_multivariate_model(trained_model_id).model_info.status\n",
    "            print(f\"model status: {model_status}\")            \n",
    "\n",
    "    def detect(self, start_time, end_time):\n",
    "        # Detect anomaly in the same data source (but a different interval)\n",
    "        try:\n",
    "            data_source = self.generate_data_source_sas(self.container, self.blob_name)\n",
    "            detection_req = DetectionRequest(source=data_source, start_time=start_time, end_time=end_time)\n",
    "            response_header = self.ad_client.detect_anomaly(self.model_id, detection_req,\n",
    "                                                            cls=lambda *args: [args[i] for i in range(len(args))])[-1]\n",
    "            result_id = response_header['Location'].split(\"/\")[-1]\n",
    "\n",
    "            # Get results (may need a few seconds)\n",
    "            r = self.ad_client.get_detection_result(result_id)\n",
    "            print(\"Get detection result...(it may take a few seconds)\")\n",
    "\n",
    "            while r.summary.status != DetectionStatus.READY and r.summary.status != DetectionStatus.FAILED:\n",
    "                r = self.ad_client.get_detection_result(result_id)\n",
    "                print(\"waiting for anomaly detection result...\")\n",
    "                time.sleep(1)\n",
    "\n",
    "            if r.summary.status == DetectionStatus.FAILED:\n",
    "                print(\"Detection failed.\")\n",
    "                if r.summary.errors:\n",
    "                    for error in r.summary.errors:\n",
    "                        print(\"Error code: {}. Message: {}\".format(error.code, error.message))\n",
    "                else:\n",
    "                    print(\"None\")\n",
    "                return None\n",
    "\n",
    "        except HttpResponseError as e:\n",
    "            print('Error code: {}'.format(e.error.code), 'Error message: {}'.format(e.error.message))\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "        return r\n",
    "\n",
    "    def list_models(self):\n",
    "        model_list = list(self.ad_client.list_multivariate_model(skip=0, top=100))\n",
    "        model_summary = pd.DataFrame([{\"model_id\": m.model_id, \"status\": m.status} for m in model_list[:50]])\n",
    "        display(model_summary)\n",
    "\n",
    "        model = model_list[0]\n",
    "        vars(model)\n",
    "\n",
    "    def generate_data_source_sas(self, container, blob_name):\n",
    "        BLOB_SAS_TEMPLATE = \"{blob_endpoint}{container_name}/{blob_name}?{sas_token}\"\n",
    "\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(conn_str=self.connection_string)\n",
    "        sas_token = generate_blob_sas(account_name=blob_service_client.account_name,\n",
    "                                    container_name=container, blob_name=blob_name,\n",
    "                                    account_key=blob_service_client.credential.account_key,\n",
    "                                    permission=BlobSasPermissions(read=True),\n",
    "                                    expiry=datetime.utcnow() + timedelta(days=1))\n",
    "        blob_sas = BLOB_SAS_TEMPLATE.format(blob_endpoint=blob_service_client.primary_endpoint,\n",
    "                                            container_name=container, blob_name=blob_name, sas_token=sas_token)\n",
    "        return blob_sas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample = MultivariateSample(anomaly_detector_endpoint, anomaly_detector_key, model_id, storage_connection_string, 'data', blob_name)\n",
    "\n",
    "if not df.empty:\n",
    "    sample.zip_data(df, zip_filename)\n",
    "    sample.upload_blob(zip_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train a model <a class=\"anchor\" id=\"train\"></a>\n",
    "\n",
    "Before you train a model, you should specify the `subscription key` and `endpoint` of your Anomaly Detector service to create an Anomaly Detector client in the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Specify the timespan of training data using `start_time` and `end_time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = df.index[0]\n",
    "end_time = df.index[-1]\n",
    "sliding_window = 50\n",
    "\n",
    "sample.train_model(start_time, end_time, sliding_window)\n",
    "\n",
    "# Hyperparameter of model - controls how much data for input\n",
    "# If the data has natural period eg weekly or daily - fit the data to the pattern with the sliding_window\n",
    "# If data changes rapid then helpful to have a longer sliding window - ideally covering a pattern\n",
    "# If the data doesn't change much then a smaller window is good - reduces processing time\n",
    "# Sliding_window controls min length of data - A sliding window of 28 means you must at least provide 28 data points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Model Status\n",
    "‚òïÔ∏èTraining process might take few minutes to few hours (depending on the data size, in this sample case it'll take you within 3 minutes), take a cup of coffee and come back then, waiting for its status to be **READY**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  List Models <a class=\"anchor\" id=\"list\"></a>\n",
    "List models that have been trained previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e66d3849ffa053565a586dec38b3bf28a35d140e923d8fe0a14c615c657862cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
