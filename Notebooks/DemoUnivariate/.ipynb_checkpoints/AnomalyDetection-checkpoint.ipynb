{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch anomaly detection with the Anomaly Detector API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this Jupyter notebook to start visualizing anomalies as a batch with the Anomaly Detector API in Python.\n",
    "\n",
    "This notebook shows you how to send a batch anomaly detection request, and vizualize the anomalies found throughout the example data set. The graph created at the end of this notebook will display the following:\n",
    "* Anomalies found throughout the data set, highlighted.\n",
    "* The expected values versus the values contained in the data set.\n",
    "* Anomaly detection boundaries \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Stream Analytics Query\n",
    "\n",
    "If you are exporting telemetry from IoT Hub/Central to ASA then this query will be a useful starting point\n",
    "\n",
    "```sql\n",
    "WITH Telemetry AS (\n",
    "    SELECT\n",
    "        deviceId as DeviceId,\n",
    "        enrichments.deviceName as DeviceName,\n",
    "        telemetry.latitude as Latitude,\n",
    "        telemetry.longitude as Longitude,\n",
    "        MAX(telemetry.temperature) AS Temperature,\n",
    "        AVG(telemetry.humidity) AS Humidity,\n",
    "        AVG(telemetry.pressure) AS Pressure,\n",
    "        System.Timestamp() as Timestamp,\n",
    "        Count(*) as Count\n",
    "    FROM [weather-eh] TIMESTAMP BY enqueuedTime\n",
    "    GROUP BY\n",
    "        deviceId,\n",
    "        enrichments.deviceName,\n",
    "        telemetry.latitude,\n",
    "        telemetry.longitude,\n",
    "        TumblingWindow(minute,4)\n",
    ")\n",
    "\n",
    "SELECT DeviceId, DeviceName, LEFT(CAST(Timestamp as nvarchar(max)), 16) as timestamp, Temperature AS value INTO [anomaly-data] FROM Telemetry\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the following Python libraries.\n",
    "\n",
    "- pandas\n",
    "- numpy\n",
    "- bokeh\n",
    "- ipywidgets\n",
    "- azure.storage.blob\n",
    "- matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start sending requests to the Anomaly Detector API, paste your Anomaly Detector resource access key below,\n",
    "# and replace the endpoint variable with the endpoint for your region or your on-premise container endpoint. \n",
    "# Endpoint examples:\n",
    "# https://westus2.api.cognitive.microsoft.com/anomalydetector/v1.0/timeseries/entire/detect\n",
    "# http://127.0.0.1:5000/anomalydetector/v1.0/timeseries/entire/detect\n",
    "\n",
    "# Output from Azure Stream Analytics\n",
    "# apikey = '[REPLACE_WITH_AD_SUBSCRIPTION_KEY]'\n",
    "# endpoint = '[REPLACE_WITH_AD_ENDPOINT_URL]'\n",
    "# blob_conn_str=\"[REPLACE_WITH_BLOB_CONNECTION_STRING]\"\n",
    "\n",
    "# IoT Central Blog Storage Logging\n",
    "apikey = \"\"\n",
    "endpoint = \"\"\n",
    "device_name = \"\"\n",
    "blob_conn_str=\"\"\n",
    "\n",
    "blob_filter = \"d89eef5e-6e74-43cf-aa04-2f36e81b91da/1/2022/07/\"\n",
    "input_container = \"workspaces\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import math\n",
    "from azure.storage.blob import ContainerClient\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import library to display results\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure,output_notebook, show\n",
    "from bokeh.palettes import Blues4\n",
    "from bokeh.models import ColumnDataSource,Slider\n",
    "import datetime\n",
    "from bokeh.io import push_notebook\n",
    "from dateutil import parser\n",
    "from ipywidgets import interact, widgets, fixed\n",
    "from IPython.display import clear_output \n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(endpoint, apikey, request_data):\n",
    "    headers = {'Content-Type': 'application/json', 'Ocp-Apim-Subscription-Key': apikey}\n",
    "    response = requests.post(endpoint, data=json.dumps(request_data), headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return json.loads(response.content.decode(\"utf-8\"))\n",
    "    else:\n",
    "        print(response.status_code)\n",
    "        raise Exception(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "format_json_data iterates through the blob data and adds to a python dictionary.\n",
    "A dictionary is used as it dedups any duplicate timestamps. The dictionary is later converted to a \n",
    "list of json objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_figure(sample_data, sensitivity):\n",
    "    sample_data['sensitivity'] = sensitivity\n",
    "    \n",
    "    result = detect(endpoint, apikey, sample_data)\n",
    "        \n",
    "    columns = {'expectedValues': result['expectedValues'], 'isAnomaly': result['isAnomaly'], 'isNegativeAnomaly': result['isNegativeAnomaly'],\n",
    "          'isPositiveAnomaly': result['isPositiveAnomaly'], 'upperMargins': result['upperMargins'], 'lowerMargins': result['lowerMargins'],\n",
    "          'timestamp': [parser.parse(x['timestamp']) for x in sample_data['series']], \n",
    "          'value': [x['value'] for x in sample_data['series']]}\n",
    "    response = pd.DataFrame(data=columns)\n",
    "    values = response['value']\n",
    "    label = response['timestamp']\n",
    "    anomalies = []\n",
    "    anomaly_labels = []\n",
    "    index = 0\n",
    "    anomaly_indexes = []\n",
    "    p = figure(x_axis_type='datetime', title=\"Batch Anomaly Detection ({0} Sensitvity)\".format(sensitivity), width=800, height=600)\n",
    "    for anom in response['isAnomaly']:\n",
    "        if anom == True and (values[index] > response.iloc[index]['expectedValues'] + response.iloc[index]['upperMargins'] or \n",
    "                         values[index] < response.iloc[index]['expectedValues'] - response.iloc[index]['lowerMargins']):\n",
    "            anomalies.append(values[index])\n",
    "            anomaly_labels.append(label[index])\n",
    "            anomaly_indexes.append(index)\n",
    "        index = index+1\n",
    "    upperband = response['expectedValues'] + response['upperMargins']\n",
    "    lowerband = response['expectedValues'] -response['lowerMargins']\n",
    "    band_x = np.append(label, label[::-1])\n",
    "    band_y = np.append(lowerband, upperband[::-1])\n",
    "    boundary = p.patch(band_x, band_y, color=Blues4[2], fill_alpha=0.5, line_width=1, legend='Boundary')\n",
    "    p.line(label, values, legend='Value', color=\"#2222aa\", line_width=1)\n",
    "    p.line(label, response['expectedValues'], legend='ExpectedValue',  line_width=1, line_dash=\"dotdash\", line_color='olivedrab')\n",
    "    anom_source = ColumnDataSource(dict(x=anomaly_labels, y=anomalies))\n",
    "    anoms = p.circle('x', 'y', size=5, color='tomato', source=anom_source)\n",
    "    p.legend.border_line_width = 1\n",
    "    p.legend.background_fill_alpha  = 0.1\n",
    "    show(p, notebook_handle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    with open('temperature.json',mode='r') as file: \n",
    "        filecontents = file.read()\n",
    "    format_json_data(filecontents, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_entries_from_log(telemetry_data):\n",
    "    parse_json_records = [json.loads(str(item)) for item in telemetry_data.strip().split('\\n') ]\n",
    "    for item in parse_json_records:\n",
    "        if item[\"enrichments\"][\"deviceName\"] == device_name:\n",
    "            temperature = item['telemetry']['temperature']\n",
    "            if item['telemetry']['temperature'] is not None and item[\"enqueuedTime\"] is not None and not math.isnan(temperature):\n",
    "                di = {}\n",
    "                di['timestamp'] = item[\"enqueuedTime\"]\n",
    "                di['value'] = temperature\n",
    "                data.append(di)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_iot_central_log(input_container):\n",
    "    container = ContainerClient.from_connection_string(\n",
    "        conn_str=blob_conn_str, container_name=input_container)\n",
    "    blob_name = ''\n",
    "    json_paths = []\n",
    "    blob_list = container.list_blobs(name_starts_with = blob_filter )\n",
    "    for blob in blob_list:\n",
    "        # read raw data from blob storage\n",
    "        blob_name = blob.name\n",
    "        # print(blob.name + '\\n')\n",
    "        blob_client = container.get_blob_client(blob_name)\n",
    "        filestream = blob_client.download_blob()\n",
    "        filecontents = filestream.content_as_text()\n",
    "\n",
    "        if filecontents:\n",
    "            add_entries_from_log(filecontents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizualizing anomalies throughout your data\n",
    "\n",
    "The following cells call the Anomaly Detector API with two different example time series data sets, and different sensitivities for anomaly detection. Varying the sensitivity of the Anomaly Detector API can improve how well the response fits your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: time series with an hourly sampling frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONT RUN AS WILL BE USING OFFLINE DATA FOR DEMO\n",
    "\n",
    "# data = []\n",
    "\n",
    "# # Get data from blob storage\n",
    "# # process_raw_data()\n",
    "# read_iot_central_log(input_container)\n",
    "\n",
    "# # Clean data\n",
    "\n",
    "# # convert list object to a pandas dataframe\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Convert datetime string to dataframe timestamp type\n",
    "# # https://cumsum.wordpress.com/2022/02/26/pandas-typeerror-only-valid-with-datetimeindex-timedeltaindex-or-periodindex-but-got-an-instance-of-index/\n",
    "# df.index = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# # Resample into 10 minute buckets\n",
    "# # https://towardsdatascience.com/using-the-pandas-resample-function-a231144194c4\n",
    "# res = df.resample('5min').max()\n",
    "\n",
    "# # remove existing timestamp column convert the datetime index to a timestamp column\n",
    "# # https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html\n",
    "# # https://stackoverflow.com/questions/44773714/convert-index-to-column-pandas-dataframe\n",
    "# res = res.drop(columns=['timestamp']).reset_index()\n",
    "\n",
    "# # Convert timestamp column of type timestamp to type string\n",
    "# res['timestamp']=res['timestamp'].astype(str)\n",
    "\n",
    "# # convert dataframe to a list of json objects\n",
    "# ts_dict = res.to_dict('records')\n",
    "\n",
    "# with open('iot_central_data_backup.json', 'w') as f:\n",
    "#     json.dump(ts_dict, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from backup if there is a network/service problem :)\n",
    "\n",
    "with open('iot_central_data_backup.json', 'r') as f:\n",
    "    ts_dict = json.load(f)\n",
    "    print(json.dumps(ts_dict, indent=4, sort_keys=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ts_dict)\n",
    "\n",
    "if len(ts_dict) < 12:\n",
    "    print(\"Climate data items: {count}\".format(count=len(ts_dict)))\n",
    "    print(\"12 Climate data items required\")\n",
    "    print(\"Wait for more data to be generated then try again\")\n",
    "else:\n",
    "\n",
    "    # Create data for anomaly detection\n",
    "\n",
    "    sample_data = {}\n",
    "    sample_data['series'] = ts_dict\n",
    "    sample_data['granularity'] = 'minutely'\n",
    "    sample_data['customInterval'] = 5\n",
    "\n",
    "    # sample_data['period'] = 24\n",
    "    # 95 sensitivity\n",
    "    build_figure(sample_data,95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90 sensitivity\n",
    "build_figure(sample_data,90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#85 sensitivity\n",
    "build_figure(sample_data,85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e66d3849ffa053565a586dec38b3bf28a35d140e923d8fe0a14c615c657862cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
