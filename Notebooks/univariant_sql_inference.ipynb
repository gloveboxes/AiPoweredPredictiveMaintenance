{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch anomaly detection with the Anomaly Detector API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this Jupyter notebook to start visualizing anomalies as a batch with the Anomaly Detector API in Python.\n",
    "\n",
    "This notebook shows you how to send a batch anomaly detection request, and vizualize the anomalies found throughout the example data set. The graph created at the end of this notebook will display the following:\n",
    "* Anomalies found throughout the data set, highlighted.\n",
    "* The expected values versus the values contained in the data set.\n",
    "* Anomaly detection boundaries \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the following Python libraries.\n",
    "\n",
    "- pandas\n",
    "- numpy\n",
    "- bokeh\n",
    "- ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select device for anomaly analysis\n",
    "\n",
    "deviceId = \"hvac_simulator\"\n",
    "inferenceDays = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages. uncomment to install\n",
    "\n",
    "# ! pip3 install notebook\n",
    "# ! pip3 install azure-ai-anomalydetector\n",
    "# ! pip3 install azure-core\n",
    "# ! pip3 install azure-storage-blob\n",
    "# ! pip3 install python-dotenv\n",
    "# ! pip3 install pandas\n",
    "# ! pip3 install plotly\n",
    "# ! pip3 install bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tempfile\n",
    "import warnings\n",
    "\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.palettes import Blues4\n",
    "from bokeh.plotting import figure,output_notebook, show\n",
    "from dateutil import parser\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "\n",
    "env_path = Path('.') / '.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "inference_telemetry_endpoint_url = os.environ.get('inference_telemetry_endpoint_url')\n",
    "inference_telemetry_endpoint_key = os.environ.get('inference_telemetry_endpoint_key')\n",
    "anomaly_detector_endpoint = os.environ.get('anomaly_detector_endpoint') + '/anomalydetector/v1.0/timeseries/entire/detect'\n",
    "anomaly_detector_key = os.environ.get('anomaly_detector_key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the anomaly detector API\n",
    "\n",
    "def detect(endpoint, apikey, request_data):\n",
    "\n",
    "    headers = {'Content-Type': 'application/json', 'Ocp-Apim-Subscription-Key': apikey}\n",
    "    response = requests.post(endpoint, data=json.dumps(request_data), headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return json.loads(response.content.decode(\"utf-8\"))\n",
    "    else:\n",
    "        print(response.status_code)\n",
    "        raise Exception(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format_json_data iterates through the blob data and adds to a python dictionary.\n",
    "# A dictionary is used as it dedups any duplicate timestamps. \n",
    "# The dictionary is later converted to a list of json objects.\n",
    "\n",
    "def build_figure(sample_data, sensitivity):\n",
    "    sample_data['sensitivity'] = sensitivity\n",
    "    \n",
    "    result = detect(anomaly_detector_endpoint, anomaly_detector_key, sample_data)\n",
    "        \n",
    "    columns = {'expectedValues': result['expectedValues'], 'isAnomaly': result['isAnomaly'], 'isNegativeAnomaly': result['isNegativeAnomaly'],\n",
    "          'isPositiveAnomaly': result['isPositiveAnomaly'], 'upperMargins': result['upperMargins'], 'lowerMargins': result['lowerMargins'],\n",
    "          'timestamp': [parser.parse(x['timestamp']) for x in sample_data['series']], \n",
    "          'value': [x['value'] for x in sample_data['series']]}\n",
    "    response = pd.DataFrame(data=columns)\n",
    "    values = response['value']\n",
    "    label = response['timestamp']\n",
    "    anomalies = []\n",
    "    anomaly_labels = []\n",
    "    index = 0\n",
    "    anomaly_indexes = []\n",
    "    p = figure(x_axis_type='datetime', title=\"Batch Anomaly Detection ({0} Sensitvity)\".format(sensitivity), width=800, height=600)\n",
    "    for anom in response['isAnomaly']:\n",
    "        if anom == True and (values[index] > response.iloc[index]['expectedValues'] + response.iloc[index]['upperMargins'] or \n",
    "                         values[index] < response.iloc[index]['expectedValues'] - response.iloc[index]['lowerMargins']):\n",
    "            anomalies.append(values[index])\n",
    "            anomaly_labels.append(label[index])\n",
    "            anomaly_indexes.append(index)\n",
    "        index = index+1\n",
    "    upperband = response['expectedValues'] + response['upperMargins']\n",
    "    lowerband = response['expectedValues'] -response['lowerMargins']\n",
    "    band_x = np.append(label, label[::-1])\n",
    "    band_y = np.append(lowerband, upperband[::-1])\n",
    "    boundary = p.patch(band_x, band_y, color=Blues4[2], fill_alpha=0.5, line_width=1, legend_label='Boundary')\n",
    "    p.line(label, values, legend_label='Value', color=\"#2222aa\", line_width=1)\n",
    "    p.line(label, response['expectedValues'], legend_label='ExpectedValue',  line_width=1, line_dash=\"dotdash\", line_color='olivedrab')\n",
    "    anom_source = ColumnDataSource(dict(x=anomaly_labels, y=anomalies))\n",
    "    anoms = p.circle('x', 'y', size=5, color='tomato', source=anom_source)\n",
    "    p.legend.border_line_width = 1\n",
    "    p.legend.background_fill_alpha  = 0.1\n",
    "    show(p, notebook_handle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizualizing anomalies throughout your data\n",
    "\n",
    "The following cells call the Anomaly Detector API with two different example time series data sets, and different sensitivities for anomaly detection. Varying the sensitivity of the Anomaly Detector API can improve how well the response fits your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: time series with an hourly sampling frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get last X days of telemetry from Azure SQL via REST API\n",
    "\n",
    "api_url = f\"{inference_telemetry_endpoint_url}/{deviceId}/{inferenceDays}?code={inference_telemetry_endpoint_key}\"\n",
    "\n",
    "ts_dict = {}\n",
    "df = pd.read_json(api_url, convert_dates=False)\n",
    "if not df.empty:\n",
    "    df.drop(columns=['humidity', 'prediction'], inplace=True, errors='raise')\n",
    "    df.rename(columns={\"temperature\": \"value\"}, inplace=True, errors='raise')\n",
    "\n",
    "    result = df.to_json(orient=\"records\")\n",
    "    ts_dict = json.loads(result)\n",
    "\n",
    "    df\n",
    "else:\n",
    "    print(f\"No data found for device {deviceId} in the last {inferenceDays} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ts_dict)\n",
    "sample_data = {}\n",
    "\n",
    "if len(ts_dict) < 12:\n",
    "    print(\"Climate data items: {count}\".format(count=len(ts_dict)))\n",
    "    print(\"12 Climate data items required\")\n",
    "    print(\"Wait for more data to be generated then try again\")\n",
    "else:\n",
    "    # build sample data for anomaly detection\n",
    "\n",
    "    \n",
    "    sample_data['series'] = ts_dict\n",
    "    sample_data['granularity'] = 'minutely'\n",
    "    sample_data['customInterval'] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95 sensitivity\n",
    "\n",
    "if len(sample_data) > 0:\n",
    "    build_figure(sample_data,95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90 sensitivity\n",
    "\n",
    "if len(sample_data) > 0:\n",
    "    build_figure(sample_data,90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#80 sensitivity\n",
    "\n",
    "if len(sample_data) > 0:\n",
    "    build_figure(sample_data,80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e66d3849ffa053565a586dec38b3bf28a35d140e923d8fe0a14c615c657862cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
